{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rmpybwysXGV"
   },
   "source": [
    "### osu!nn #7: GAN map flow generator\n",
    "\n",
    "Generate a nice map using a GAN and the data we have gathered until now.\n",
    "\n",
    "Synthesis of \"flowData\"\n",
    "* training_flowData x 10 ~ 99 (Quality: 60+)\n",
    "* rhythmData x 1\n",
    "* momentumData x 1\n",
    "* (Discriminator) x 1\n",
    "* (Generator) x 1\n",
    "\n",
    "Synthesis Time: ~15 mins\n",
    "\n",
    "Last edit: 2019/4/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m8y3rGtQsYP2"
   },
   "source": [
    "#### First of all, let's welcome -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "## Cute Sophie!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7S0BwJ_8sLu7"
   },
   "source": [
    "<img src=\"https://ar3.moe/files/sophie.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2o3TTG4TFpt"
   },
   "source": [
    "In the previous notebook we have predicted... or estimated our rhythm, and now we will be trying to create the new map by imitating the existing dataset, using a Generative Adversial Network (GAN).\n",
    "\n",
    "Note that this GAN is irrelevant to the music, not the time interval, only the coordinates of notes themselves.\n",
    "\n",
    "Probably could get some slider coordinates inbetween? this way it may learn something about slider shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PJ64L90aVir3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, subprocess, json\n",
    "from datetime import datetime\n",
    "\n",
    "# Tensorflow 2.0 enables eager automatically\n",
    "try:\n",
    "    tf.enable_eager_execution();\n",
    "    tfe = tf.contrib.eager;\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters related to GAN training.\n",
    "\n",
    "May or may not affect the result; haven't experimented a lot about these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_PARAMS = {\n",
    "    \"divisor\" : 4,\n",
    "    \"good_epoch\" : 6,\n",
    "    \"max_epoch\" : 25,\n",
    "    \"note_group_size\" : 10,\n",
    "    \"g_epochs\" : 7,\n",
    "    \"c_epochs\" : 3,\n",
    "    \"g_batch\" : 50,\n",
    "    \"g_input_size\" : 50,\n",
    "    \"c_true_batch\" : 50,\n",
    "    \"c_false_batch\" : 10,\n",
    "    \"slider_max_ticks\" : 8,\n",
    "    \"next_from_slider_end\" : False\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "hBkaqxc2TsrW"
   },
   "source": [
    "#### Import the rhythm data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npz(fn):\n",
    "    with np.load(fn) as data:\n",
    "        objs = data[\"objs\"];\n",
    "        obj_indices = [i for i,k in enumerate(objs) if k == 1];\n",
    "        predictions = data[\"predictions\"];\n",
    "        momenta = data[\"momenta\"];\n",
    "        ticks = data[\"ticks\"];\n",
    "        timestamps = data[\"timestamps\"];\n",
    "        sv = data[\"sv\"];\n",
    "        dist_multiplier = data[\"dist_multiplier\"];\n",
    "    return objs, obj_indices, predictions, momenta, ticks, timestamps, sv, dist_multiplier;\n",
    "\n",
    "unfiltered_objs, obj_indices, unfiltered_predictions, unfiltered_momenta, unfiltered_ticks, unfiltered_timestamps, unfiltered_sv, dist_multiplier = read_npz(\"rhythm_data.npz\");\n",
    "\n",
    "first_step_objs =        unfiltered_objs[obj_indices];\n",
    "first_step_predictions = unfiltered_predictions[obj_indices];\n",
    "first_step_momenta =     unfiltered_momenta[obj_indices];\n",
    "first_step_ticks =       unfiltered_ticks[obj_indices];\n",
    "first_step_timestamps =  unfiltered_timestamps[obj_indices];\n",
    "first_step_sv =          unfiltered_sv[obj_indices];\n",
    "\n",
    "momentum_multiplier = 1.0;\n",
    "angular_momentum_multiplier = 1.0;\n",
    "\n",
    "first_step_is_slider = first_step_predictions[:, 2];\n",
    "first_step_is_spinner = first_step_predictions[:, 3];\n",
    "first_step_is_sliding = first_step_predictions[:, 4];\n",
    "first_step_is_spinning = first_step_predictions[:, 5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert notes with is_slider flag to sliders\n",
    "# if there is next note, slide to next note\n",
    "# else, slide for 4 ticks\n",
    "\n",
    "# Problems:\n",
    "# - how to deal with momenta? (change the slider speed! which is obviously not good)\n",
    "# - do we use AM?\n",
    "# - do we use is_sliding?\n",
    "# - do we use the slider model? it's heavily overfit... (try with some other dataset, other than sota!!!)\n",
    "# - does the shape cause overlapping? add some penalty loss? let it learn from classifier? ...\n",
    "# - and many more!\n",
    "skip_this = False;\n",
    "new_obj_indices = [];\n",
    "slider_ticks = [];\n",
    "slider_max_ticks = GAN_PARAMS[\"slider_max_ticks\"];\n",
    "for i in range(len(first_step_objs)):\n",
    "    if skip_this:\n",
    "        first_step_is_slider[i] = 0;\n",
    "        skip_this = False;\n",
    "        continue;\n",
    "    if first_step_is_slider[i]: # this one is a slider!!\n",
    "        if i == first_step_objs.shape[0]-1: # Last Note.\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(slider_max_ticks);\n",
    "            continue;\n",
    "        if first_step_ticks[i+1] >= first_step_ticks[i] + slider_max_ticks + 1: # too long! end here\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(slider_max_ticks);\n",
    "        else:\n",
    "            skip_this = True;\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(max(1, first_step_ticks[i+1] - first_step_ticks[i]));\n",
    "    else: # not a slider!\n",
    "        new_obj_indices.append(i);\n",
    "        slider_ticks.append(0);\n",
    "\n",
    "# Filter the removed objects out!\n",
    "objs =        first_step_objs[new_obj_indices];\n",
    "predictions = first_step_predictions[new_obj_indices];\n",
    "momenta =     first_step_momenta[new_obj_indices];\n",
    "ticks =       first_step_ticks[new_obj_indices];\n",
    "timestamps =  first_step_timestamps[new_obj_indices];\n",
    "is_slider =   first_step_is_slider[new_obj_indices];\n",
    "is_spinner =  first_step_is_spinner[new_obj_indices];\n",
    "is_sliding =  first_step_is_sliding[new_obj_indices];\n",
    "is_spinning = first_step_is_spinning[new_obj_indices];\n",
    "sv =          first_step_sv[new_obj_indices];\n",
    "slider_ticks = np.array(slider_ticks);\n",
    "\n",
    "# get divisor from GAN_PARAMS\n",
    "divisor = GAN_PARAMS[\"divisor\"];\n",
    "\n",
    "# should be slider length each tick, which is usually SV * SMP * 100 / 4\n",
    "# e.g. SV 1.6, timing section x1.00, 1/4 divisor, then slider_length_base = 40\n",
    "slider_length_base = sv // divisor;\n",
    "\n",
    "# these data must be kept consistent with the sliderTypes in load_map.js\n",
    "slider_types = np.random.randint(0, 5, is_slider.shape).astype(int); # needs to determine the slider types!! also it is 5!!!\n",
    "slider_type_rotation = np.array([0, -0.40703540572409336, 0.40703540572409336, -0.20131710837464062, 0.20131710837464062]);\n",
    "slider_cos = np.cos(slider_type_rotation);\n",
    "slider_sin = np.sin(slider_type_rotation);\n",
    "\n",
    "slider_cos_each = slider_cos[slider_types];\n",
    "slider_sin_each = slider_sin[slider_types];\n",
    "\n",
    "# this is vector length! I should change the variable name probably...\n",
    "slider_type_length = np.array([1.0, 0.97, 0.97, 0.97, 0.97]);\n",
    "\n",
    "slider_lengths = np.array([slider_type_length[int(k)] * slider_length_base[i] for i, k in enumerate(slider_types)]) * slider_ticks;\n",
    "\n",
    "# print(slider_lengths.shape)\n",
    "# print(timestamps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For distances starting from slider ends\n",
    "tick_lengths_pre = (timestamps[1:] - timestamps[:-1]) / (ticks[1:] - ticks[:-1]);\n",
    "tick_lengths = np.concatenate([tick_lengths_pre, [tick_lengths_pre[-1]]]);\n",
    "timestamps_note_end = timestamps + slider_ticks * tick_lengths;\n",
    "\n",
    "timestamps_plus_1 = np.concatenate([timestamps[1:], timestamps[-1:] + (timestamps[-1:] - timestamps[-2:-1])])\n",
    "\n",
    "if GAN_PARAMS[\"next_from_slider_end\"]:\n",
    "    timestamps_after = timestamps_plus_1 - timestamps_note_end;\n",
    "    timestamps_before = np.concatenate([[6662], timestamps_after[:-1]]); # why 6662????\n",
    "    note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "else:\n",
    "    timestamps_after = timestamps_plus_1 - timestamps;\n",
    "    timestamps_before = np.concatenate([[4777], timestamps_after[:-1]]); # why 4777????\n",
    "    note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "note_angles = timestamps_before * momenta[:, 1] * angular_momentum_multiplier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_slider = predictions[:, 2];\n",
    "is_sliding = predictions[:, 4];\n",
    "#print(is_slider * is_sliding - is_slider); # is all 0!!\n",
    "# print(is_slider * is_sliding);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting functions (only for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VAkAvc3ATskA"
   },
   "outputs": [],
   "source": [
    "from plthelper import MyLine, plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"mapdata/\";\n",
    "\n",
    "chunk_size = GAN_PARAMS[\"note_group_size\"];\n",
    "step_size = 5;\n",
    "\n",
    "max_x = 512;\n",
    "max_y = 384;\n",
    "\n",
    "# \"TICK\", \"TIME\", \"TYPE\", \"X\", \"Y\", \"IN_DX\", \"IN_DY\", \"OUT_DX\", \"OUT_DY\"\n",
    "def read_map_npz(file_path):\n",
    "    with np.load(file_path) as data:\n",
    "        flow_data = data[\"flow\"];\n",
    "    return flow_data;\n",
    "\n",
    "# TICK, TIME, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY\n",
    "def read_maps():\n",
    "    result = [];\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            #print(os.path.join(root, file));\n",
    "            flow_data = read_map_npz(os.path.join(root, file));\n",
    "            for i in range(0, (flow_data.shape[0] - chunk_size) // step_size):\n",
    "                chunk = flow_data[i * step_size:i * step_size + chunk_size];\n",
    "                result.append(chunk);\n",
    "                \n",
    "    # normalize the TICK col and remove TIME col\n",
    "    result = np.array(result)[:, :, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10]];\n",
    "    result[:, :, 0] %= divisor;\n",
    "    result[:, :, 2] /= max_x;\n",
    "    result[:, :, 3] /= max_y;\n",
    "    result[:, :, 8] /= max_x;\n",
    "    result[:, :, 9] /= max_y;\n",
    "    \n",
    "    # TICK, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY, END_X, END_Y\n",
    "    # only use X,Y,OUT_DX,OUT_DY,END_X,END_Y\n",
    "    result = np.array(result)[:, :, [2, 3, 6, 7, 8, 9]];\n",
    "    return result;\n",
    "\n",
    "# The default dataset so people don't have to come up with a whole dataset to use this.\n",
    "# To save the flow data to a flow_dataset.npz, it is simple - just run the following after reading maps:\n",
    "# np.savez_compressed(\"flow_dataset\", maps = maps);\n",
    "try:\n",
    "    maps = read_maps();\n",
    "    labels = np.ones(maps.shape[0]);\n",
    "except:\n",
    "    with np.load(\"flow_dataset.npz\") as flow_dataset:\n",
    "        maps = flow_dataset[\"maps\"];\n",
    "        labels = np.ones(maps.shape[0]);\n",
    "\n",
    "order2 = np.argsort(np.random.random(maps.shape[0]));\n",
    "special_train_data = maps[order2];\n",
    "special_train_labels = labels[order2];\n",
    "# order3 = np.argsort(np.random.random(false_maps.shape[0]));\n",
    "# special_false_data = false_maps[order2];\n",
    "# special_false_labels = false_labels[order2];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the classifier model.\n",
    "\n",
    "The model structure can be probably optimized... while I currently have no good idea about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras;\n",
    "\n",
    "def build_classifier_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.SimpleRNN(64, input_shape=(special_train_data.shape[1], special_train_data.shape[2])),\n",
    "        keras.layers.Dense(64),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(64, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.tanh),\n",
    "        keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(1,)),\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        optimizer = tf.optimizers.Adam(0.001) #Adamoptimizer?\n",
    "    except:\n",
    "        optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qutT_fkl_CBc"
   },
   "source": [
    "Functions for map generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gxPTb-kt_N5m"
   },
   "outputs": [],
   "source": [
    "from tfhelper import *\n",
    "\n",
    "# A regularizer to keep the map inside the box.\n",
    "# It's so the sliders and notes don't randomly fly out of the screen!\n",
    "def inblock_loss(vg):\n",
    "    wall_var_l = tf.where(tf.less(vg, 0.2), tf.square(0.3 - vg), 0 * vg);\n",
    "    wall_var_r = tf.where(tf.greater(vg, 0.8), tf.square(vg - 0.7), 0 * vg);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def inblock_trueness(vg):\n",
    "    wall_var_l = tf.cast(tf.less(vg, 0), tf.float32);\n",
    "    wall_var_r = tf.cast(tf.greater(vg, 1), tf.float32);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def cut_map_chunks(c):\n",
    "    r = [];\n",
    "    for i in range(0, (c.shape[0] - chunk_size) // step_size):\n",
    "        chunk = c[i * step_size:i * step_size + chunk_size];\n",
    "        r.append(chunk);\n",
    "    return tf.stack(r);\n",
    "\n",
    "def construct_map_with_sliders(var_tensor, extvar=[]):\n",
    "    var_tensor = tf.cast(var_tensor, tf.float32);\n",
    "    var_shape = var_tensor.shape;\n",
    "    wall_l = 0.15;\n",
    "    wall_r = 0.85;\n",
    "    x_max = 512;\n",
    "    y_max = 384;\n",
    "    out = [];\n",
    "    cp = tf.constant([256, 192, 0, 0]);\n",
    "    phase = 0;\n",
    "    half_tensor = var_shape[1]//4;\n",
    "    \n",
    "    # length multiplier\n",
    "    if \"length_multiplier\" in extvar:\n",
    "        length_multiplier = extvar[\"length_multiplier\"];\n",
    "    else:\n",
    "        length_multiplier = 1;\n",
    "\n",
    "    # notedists\n",
    "    if \"begin\" in extvar:\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "    else:\n",
    "        begin_offset = 0;\n",
    "    \n",
    "#     note_distances_now = length_multiplier * np.expand_dims(note_distances[begin_offset:begin_offset+half_tensor], axis=0);\n",
    "#     note_angles_now = np.expand_dims(note_angles[begin_offset:begin_offset+half_tensor], axis=0);\n",
    "    \n",
    "    relevant_tensors = extvar[\"relevant_tensors\"];\n",
    "    relevant_is_slider =      relevant_tensors[\"is_slider\"];\n",
    "    relevant_slider_lengths = relevant_tensors[\"slider_lengths\"];\n",
    "    relevant_slider_types =   relevant_tensors[\"slider_types\"];\n",
    "    relevant_slider_cos =     relevant_tensors[\"slider_cos_each\"];\n",
    "    relevant_slider_sin =     relevant_tensors[\"slider_sin_each\"];\n",
    "    relevant_note_distances = relevant_tensors[\"note_distances\"];\n",
    "    relevant_note_angles =    relevant_tensors[\"note_angles\"];\n",
    "    \n",
    "    note_distances_now = length_multiplier * tf.expand_dims(relevant_note_distances, axis=0);\n",
    "    note_angles_now = tf.expand_dims(relevant_note_angles, axis=0);\n",
    "\n",
    "    # init\n",
    "    l = tf.convert_to_tensor(note_distances_now, dtype=\"float32\");\n",
    "    sl = l * 0.7;\n",
    "    sr = tf.convert_to_tensor(note_angles_now, dtype=\"float32\");\n",
    "    \n",
    "    cos_list = var_tensor[:, 0:half_tensor * 2];\n",
    "    sin_list = var_tensor[:, half_tensor * 2:];\n",
    "    len_list = tf.sqrt(tf.square(cos_list) + tf.square(sin_list));\n",
    "    cos_list = cos_list / len_list;\n",
    "    sin_list = sin_list / len_list;\n",
    "    \n",
    "    wall_l = 0.05 * x_max + l * 0.5;\n",
    "    wall_r = 0.95 * x_max - l * 0.5;\n",
    "    wall_t = 0.05 * y_max + l * 0.5;\n",
    "    wall_b = 0.95 * y_max - l * 0.5;\n",
    "    rerand = tf.cast(tf.greater(l, y_max / 2), tf.float32);\n",
    "    not_rerand = tf.cast(tf.less_equal(l, y_max / 2), tf.float32);\n",
    "    \n",
    "    next_from_slider_end = extvar[\"next_from_slider_end\"];\n",
    "\n",
    "    # generate\n",
    "    if \"start_pos\" in extvar:\n",
    "        _pre_px = extvar[\"start_pos\"][0];\n",
    "        _pre_py = extvar[\"start_pos\"][1];\n",
    "        _px = tf.cast(_pre_px, tf.float32);\n",
    "        _py = tf.cast(_pre_py, tf.float32);\n",
    "    else:\n",
    "        _px = tf.cast(256, tf.float32);\n",
    "        _py = tf.cast(192, tf.float32);\n",
    "    \n",
    "    # this is not important since the first position starts at _ppos + Δpos\n",
    "    _x = tf.cast(256, tf.float32);\n",
    "    _y = tf.cast(192, tf.float32);\n",
    "    \n",
    "    outputs = tf.TensorArray(tf.float32, half_tensor)\n",
    "\n",
    "    for k in range(half_tensor):\n",
    "        # r_max = 192, r = 192 * k, theta = k * 10\n",
    "        rerand_x = 256 + 256 * var_tensor[:, k];\n",
    "        rerand_y = 192 + 192 * var_tensor[:, k + half_tensor*2];\n",
    "\n",
    "        delta_value_x = l[:, k] * cos_list[:, k];\n",
    "        delta_value_y = l[:, k] * sin_list[:, k];\n",
    "\n",
    "        # It is tensor calculation batched 8~32 each call, so if/else do not work here.\n",
    "        wall_value_l =    tf.cast(tf.less(_px, wall_l[:, k]), tf.float32);\n",
    "        wall_value_r =    tf.cast(tf.greater(_px, wall_r[:, k]), tf.float32);\n",
    "        wall_value_xmid = tf.cast(tf.greater(_px, wall_l[:, k]), tf.float32) * tf.cast(tf.less(_px, wall_r[:, k]), tf.float32);\n",
    "        wall_value_t =    tf.cast(tf.less(_py, wall_t[:, k]), tf.float32);\n",
    "        wall_value_b =    tf.cast(tf.greater(_py, wall_b[:, k]), tf.float32);\n",
    "        wall_value_ymid = tf.cast(tf.greater(_py, wall_t[:, k]), tf.float32) * tf.cast(tf.less(_py, wall_b[:, k]), tf.float32);\n",
    "\n",
    "        x_delta = tf.abs(delta_value_x) * wall_value_l - tf.abs(delta_value_x) * wall_value_r + delta_value_x * wall_value_xmid;\n",
    "        y_delta = tf.abs(delta_value_y) * wall_value_t - tf.abs(delta_value_y) * wall_value_b + delta_value_y * wall_value_ymid;\n",
    "\n",
    "        _x = rerand[:, k] * rerand_x + not_rerand[:, k] * (_px + x_delta);\n",
    "        _y = rerand[:, k] * rerand_y + not_rerand[:, k] * (_py + y_delta);\n",
    "#         _x = _px + x_delta;\n",
    "#         _y = _py + y_delta;\n",
    "        \n",
    "        # calculate output vector\n",
    "        \n",
    "        # slider part\n",
    "        sln = relevant_slider_lengths[k];\n",
    "        slider_type = relevant_slider_types[k];\n",
    "        scos = relevant_slider_cos[k];\n",
    "        ssin = relevant_slider_sin[k];\n",
    "        _a = cos_list[:, k + half_tensor];\n",
    "        _b = sin_list[:, k + half_tensor];\n",
    "        # cos(a+θ) = cosa cosθ - sina sinθ\n",
    "        # sin(a+θ) = cosa sinθ + sina cosθ\n",
    "        _oa = _a * scos - _b * ssin;\n",
    "        _ob = _a * ssin + _b * scos;\n",
    "        cp_slider = tf.transpose(tf.stack([_x / x_max, _y / y_max, _oa, _ob, (_x + _a * sln) / x_max, (_y + _b * sln) / y_max]));\n",
    "        _px_slider = tf.cond(next_from_slider_end, lambda: _x + _a * sln, lambda: _x);\n",
    "        _py_slider = tf.cond(next_from_slider_end, lambda: _y + _b * sln, lambda: _y);\n",
    "        \n",
    "        # circle part\n",
    "        _a = rerand[:, k] * cos_list[:, k + half_tensor] + not_rerand[:, k] * cos_list[:, k];\n",
    "        _b = rerand[:, k] * sin_list[:, k + half_tensor] + not_rerand[:, k] * sin_list[:, k];\n",
    "        cp_circle = tf.transpose(tf.stack([_x / x_max, _y / y_max, _a, _b, _x / x_max, _y / y_max]));\n",
    "        _px_circle = _x;\n",
    "        _py_circle = _y;\n",
    "        \n",
    "        outputs = outputs.write(k, tf.where(relevant_is_slider[k], cp_slider, cp_circle))\n",
    "        _px = tf.where(tf.cast(relevant_is_slider[k], tf.bool), _px_slider, _px_circle)\n",
    "        _py = tf.where(tf.cast(relevant_is_slider[k], tf.bool), _py_slider, _py_circle)\n",
    "\n",
    "    return tf.transpose(outputs.stack(), [1, 0, 2]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import losses_utils\n",
    "from tensorflow.python.keras.losses import LossFunctionWrapper\n",
    "\n",
    "# Loss functions and mapping layer, to adapt to TF 2.0\n",
    "class GenerativeCustomLoss(LossFunctionWrapper):\n",
    "    def __init__(self,\n",
    "        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "        name='generative_custom_loss'):\n",
    "        \n",
    "        def loss_function_for_generative_model(y_true, y_pred):\n",
    "            classification = y_pred;\n",
    "            loss1 = 1 - tf.reduce_mean(classification, axis=1);\n",
    "            return loss1;\n",
    "        \n",
    "        super(GenerativeCustomLoss, self).__init__(loss_function_for_generative_model, name=name, reduction=reduction)\n",
    "\n",
    "class BoxCustomLoss(LossFunctionWrapper):\n",
    "    def __init__(self,\n",
    "        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "        name='generative_custom_loss'):\n",
    "        \n",
    "        def box_loss(y_true, y_pred):\n",
    "            map_part = y_pred;\n",
    "            return inblock_loss(map_part[:, :, 0:2]) + inblock_loss(map_part[:, :, 4:6])\n",
    "        \n",
    "        super(BoxCustomLoss, self).__init__(box_loss, name=name, reduction=reduction)\n",
    "\n",
    "class AlwaysZeroCustomLoss(LossFunctionWrapper): # why does TF not include this! this is very important in certain situations\n",
    "    def __init__(self,\n",
    "        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "        name='generative_custom_loss'):\n",
    "        \n",
    "        def alw_zero(y_true, y_pred):\n",
    "            return tf.convert_to_tensor(0, dtype=tf.float32);\n",
    "        \n",
    "        super(AlwaysZeroCustomLoss, self).__init__(alw_zero, name=name, reduction=reduction)\n",
    "        \n",
    "        \n",
    "class KerasCustomMappingLayer(keras.layers.Layer):\n",
    "    def __init__(self, extvar, output_shape=(special_train_data.shape[1], special_train_data.shape[2]), *args, **kwargs):\n",
    "        self.extvar = extvar\n",
    "        self._output_shape = output_shape\n",
    "        self.extvar_begin = tf.Variable(tf.convert_to_tensor(extvar[\"begin\"], dtype=tf.int32), trainable=False)\n",
    "        self.extvar_lmul =  tf.Variable(tf.convert_to_tensor([extvar[\"length_multiplier\"]], dtype=tf.float32), trainable=False)\n",
    "        self.extvar_nfse =  tf.Variable(tf.convert_to_tensor(extvar[\"next_from_slider_end\"], dtype=tf.bool), trainable=False)\n",
    "        self.note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "        \n",
    "        self.extvar_spos =  tf.Variable(tf.cast(tf.zeros((2, )), tf.float32), trainable=False)\n",
    "        self.extvar_rel =   tf.Variable(tf.cast(tf.zeros((7, self.note_group_size)), tf.float32), trainable=False)\n",
    "        \n",
    "        super(KerasCustomMappingLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape): # since this is a static layer, no building is required\n",
    "        pass\n",
    "    \n",
    "    def set_extvar(self, extvar):\n",
    "        self.extvar = extvar;\n",
    "        \n",
    "        # Populate extvar with the rel variable (this will modify the input extvar)\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "        self.extvar[\"relevant_tensors\"] = {\n",
    "            \"is_slider\"       : tf.convert_to_tensor(is_slider      [begin_offset : begin_offset + self.note_group_size], dtype=tf.bool),\n",
    "            \"slider_lengths\"  : tf.convert_to_tensor(slider_lengths [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"slider_types\"    : tf.convert_to_tensor(slider_types   [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"slider_cos_each\" : tf.convert_to_tensor(slider_cos_each[begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"slider_sin_each\" : tf.convert_to_tensor(slider_sin_each[begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"note_distances\" :  tf.convert_to_tensor(note_distances [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"note_angles\" :     tf.convert_to_tensor(note_angles    [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32)\n",
    "        };\n",
    "        \n",
    "        # Continue\n",
    "        self.extvar_begin.assign(extvar[\"begin\"])\n",
    "        self.extvar_spos.assign(extvar[\"start_pos\"])\n",
    "        self.extvar_lmul.assign([extvar[\"length_multiplier\"]])\n",
    "        self.extvar_nfse.assign(extvar[\"next_from_slider_end\"])\n",
    "        self.extvar_rel.assign(tf.convert_to_tensor([\n",
    "            is_slider      [begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_lengths [begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_types   [begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_cos_each[begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_sin_each[begin_offset : begin_offset + self.note_group_size],\n",
    "            note_distances [begin_offset : begin_offset + self.note_group_size],\n",
    "            note_angles    [begin_offset : begin_offset + self.note_group_size]\n",
    "        ], dtype=tf.float32))\n",
    "\n",
    "    # Call method will sometimes get used in graph mode,\n",
    "    # training will get turned into a tensor\n",
    "#     @tf.function\n",
    "    def call(self, inputs, training=None):\n",
    "        mapvars = inputs;\n",
    "        start_pos = self.extvar_spos\n",
    "        rel = self.extvar_rel\n",
    "        extvar = {\n",
    "            \"begin\" : self.extvar_begin,\n",
    "            # \"start_pos\" : self.extvar_start_pos,\n",
    "            \"start_pos\" : tf.cast(start_pos, tf.float32),\n",
    "            \"length_multiplier\" : self.extvar_lmul,\n",
    "            \"next_from_slider_end\" : self.extvar_nfse,\n",
    "            # \"relevant_tensors\" : self.extvar_rel\n",
    "            \"relevant_tensors\" : {\n",
    "                \"is_slider\"       : tf.cast(rel[0], tf.bool),\n",
    "                \"slider_lengths\"  : tf.cast(rel[1], tf.float32),\n",
    "                \"slider_types\"    : tf.cast(rel[2], tf.float32),\n",
    "                \"slider_cos_each\" : tf.cast(rel[3], tf.float32),\n",
    "                \"slider_sin_each\" : tf.cast(rel[4], tf.float32),\n",
    "                \"note_distances\"  : tf.cast(rel[5], tf.float32),\n",
    "                \"note_angles\"     : tf.cast(rel[6], tf.float32)\n",
    "            }\n",
    "        }\n",
    "        result = construct_map_with_sliders(mapvars, extvar=extvar);\n",
    "        return result;\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwWPaJryD2aN"
   },
   "source": [
    "Now we can train the model!\n",
    "\n",
    "This will take some time. It splits map into groups of 10 (10 by default), and trains GAN to play with the flow of maps in the dataset. It will train for floor(note_count / 10) groups, for example, if you have 820 then it needs to train 82 groups.\n",
    "\n",
    "Some notes are converted to sliders, so it has less than the predicted count in #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XdfkR223D9dW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Stuff\\miniconda\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:255: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "# of groups: 60\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "Group 0, Epoch 1: G loss: 0.21445559859275817 vs. C loss: 0.13123833677834934\n",
      "Group 0, Epoch 2: G loss: 0.3866226272923606 vs. C loss: 0.17033298313617706\n",
      "Group 0, Epoch 3: G loss: 0.4025447377136775 vs. C loss: 0.18901147031121787\n",
      "Group 0, Epoch 4: G loss: 0.2213313436933926 vs. C loss: 0.18181548350387153\n",
      "Group 0, Epoch 5: G loss: 0.2333494573831558 vs. C loss: 0.18485977086755967\n",
      "Group 0, Epoch 6: G loss: 0.25452207837785995 vs. C loss: 0.1567367066939672\n",
      "Group 0, Epoch 7: G loss: 0.42353301942348487 vs. C loss: 0.23817182746198443\n",
      "Group 0, Epoch 8: G loss: 0.11209662365061897 vs. C loss: 0.19438830349180433\n",
      "Group 1, Epoch 1: G loss: 0.22078139356204443 vs. C loss: 0.2031493882338206\n",
      "Group 1, Epoch 2: G loss: 0.20817368711744036 vs. C loss: 0.12304204454024632\n",
      "Group 1, Epoch 3: G loss: 0.6101132324763707 vs. C loss: 0.06475889848338233\n",
      "Group 1, Epoch 4: G loss: 0.38827305265835355 vs. C loss: 0.12798305021391973\n",
      "Group 1, Epoch 5: G loss: 0.4988901919552258 vs. C loss: 0.23342472314834595\n",
      "Group 1, Epoch 6: G loss: 0.03062791654041835 vs. C loss: 0.20536818189753425\n",
      "Group 2, Epoch 1: G loss: 0.217147924218859 vs. C loss: 0.18814740081628165\n",
      "Group 2, Epoch 2: G loss: 0.22191304436751774 vs. C loss: 0.15640869074397615\n",
      "Group 2, Epoch 3: G loss: 0.37965875906603674 vs. C loss: 0.1279906862311893\n",
      "Group 2, Epoch 4: G loss: 0.3861294337681361 vs. C loss: 0.2085886204408275\n",
      "Group 2, Epoch 5: G loss: 0.07530463220817703 vs. C loss: 0.19802069664001465\n",
      "Group 2, Epoch 6: G loss: 0.0689782022365502 vs. C loss: 0.1974834849437078\n",
      "Group 3, Epoch 1: G loss: 0.25193717990602765 vs. C loss: 0.1986397529641787\n",
      "Group 3, Epoch 2: G loss: 0.1740691351039069 vs. C loss: 0.1732820429735714\n",
      "Group 3, Epoch 3: G loss: 0.18280891605785915 vs. C loss: 0.151172313425276\n",
      "Group 3, Epoch 4: G loss: 0.5160571532590048 vs. C loss: 0.08782517910003662\n",
      "Group 3, Epoch 5: G loss: 0.5602083921432495 vs. C loss: 0.05435589369800356\n",
      "Group 3, Epoch 6: G loss: 0.8217051114354815 vs. C loss: 0.03891285819311937\n",
      "Group 3, Epoch 7: G loss: 0.7647298165730066 vs. C loss: 0.08392632959617509\n",
      "Group 3, Epoch 8: G loss: 0.877355328627995 vs. C loss: 0.049398832230104335\n",
      "Group 4, Epoch 1: G loss: 0.1754494650023324 vs. C loss: 0.2096963052948316\n",
      "Group 4, Epoch 2: G loss: 0.16268694145338874 vs. C loss: 0.1350017943316036\n",
      "Group 4, Epoch 3: G loss: 0.5217894801071712 vs. C loss: 0.11551504168245529\n",
      "Group 4, Epoch 4: G loss: 0.27897653750010903 vs. C loss: 0.18171933790047964\n",
      "Group 4, Epoch 5: G loss: 0.32032567518098015 vs. C loss: 0.10108371782633994\n",
      "Group 4, Epoch 6: G loss: 0.42992054564612253 vs. C loss: 0.08037225074238247\n",
      "Group 4, Epoch 7: G loss: 0.7920867953981672 vs. C loss: 0.04059607763257291\n",
      "Group 5, Epoch 1: G loss: 0.21807889768055508 vs. C loss: 0.19329403671953413\n",
      "Group 5, Epoch 2: G loss: 0.23912247249058313 vs. C loss: 0.135265801101923\n",
      "Group 5, Epoch 3: G loss: 0.3934129995959146 vs. C loss: 0.11885018895069756\n",
      "Group 5, Epoch 4: G loss: 0.30602766935314446 vs. C loss: 0.19485540315508842\n",
      "Group 5, Epoch 5: G loss: 0.06258507541247776 vs. C loss: 0.2034502625465393\n",
      "Group 5, Epoch 6: G loss: 0.0402377436203616 vs. C loss: 0.18350244892968073\n",
      "Group 6, Epoch 1: G loss: 0.19538512357643673 vs. C loss: 0.21320076286792755\n",
      "Group 6, Epoch 2: G loss: 0.18008135812623163 vs. C loss: 0.15581381817658743\n",
      "Group 6, Epoch 3: G loss: 0.48462298767907275 vs. C loss: 0.07673943291107814\n",
      "Group 6, Epoch 4: G loss: 0.4089131883212498 vs. C loss: 0.14288062933418486\n",
      "Group 6, Epoch 5: G loss: 0.3274711104375975 vs. C loss: 0.21905799044503105\n",
      "Group 6, Epoch 6: G loss: 0.23672658162457605 vs. C loss: 0.1233412283990118\n",
      "Group 6, Epoch 7: G loss: 0.6018002927303314 vs. C loss: 0.11343939354022346\n",
      "Group 7, Epoch 1: G loss: 0.20929625332355498 vs. C loss: 0.21651821666293672\n",
      "Group 7, Epoch 2: G loss: 0.12414253247635705 vs. C loss: 0.19068300061755708\n",
      "Group 7, Epoch 3: G loss: 0.09122236647776194 vs. C loss: 0.18516811066203645\n",
      "Group 7, Epoch 4: G loss: 0.1075806098324912 vs. C loss: 0.16831011904610527\n",
      "Group 7, Epoch 5: G loss: 0.26552111080714635 vs. C loss: 0.15320415960417855\n",
      "Group 7, Epoch 6: G loss: 0.3334320494106837 vs. C loss: 0.11891431113084157\n",
      "Group 8, Epoch 1: G loss: 0.19124527275562286 vs. C loss: 0.21051358845498827\n",
      "Group 8, Epoch 2: G loss: 0.15599178884710585 vs. C loss: 0.1558817558818393\n",
      "Group 8, Epoch 3: G loss: 0.3579062529972621 vs. C loss: 0.13329390353626674\n",
      "Group 8, Epoch 4: G loss: 0.2824318217379706 vs. C loss: 0.18607149355941352\n",
      "Group 8, Epoch 5: G loss: 0.09830631379570279 vs. C loss: 0.17040304674042595\n",
      "Group 8, Epoch 6: G loss: 0.3117177848305021 vs. C loss: 0.11810328252613544\n",
      "Group 9, Epoch 1: G loss: 0.1985095632927758 vs. C loss: 0.21730796164936492\n",
      "Group 9, Epoch 2: G loss: 0.15891753946031842 vs. C loss: 0.15996312267250487\n",
      "Group 9, Epoch 3: G loss: 0.23076673958982738 vs. C loss: 0.1522712426053153\n",
      "Group 9, Epoch 4: G loss: 0.2701946982315609 vs. C loss: 0.20194923215442231\n",
      "Group 9, Epoch 5: G loss: 0.1092960014939308 vs. C loss: 0.18912525143888262\n",
      "Group 9, Epoch 6: G loss: 0.20718014666012358 vs. C loss: 0.14140766196780735\n",
      "Group 10, Epoch 1: G loss: 0.2358710587024689 vs. C loss: 0.18991926312446594\n",
      "Group 10, Epoch 2: G loss: 0.25492796983037674 vs. C loss: 0.15777614050441316\n",
      "Group 10, Epoch 3: G loss: 0.22926559959139142 vs. C loss: 0.18208987017472586\n",
      "Group 10, Epoch 4: G loss: 0.23714422924177986 vs. C loss: 0.17561071945561302\n",
      "Group 10, Epoch 5: G loss: 0.23963466797556193 vs. C loss: 0.1860767656730281\n",
      "Group 10, Epoch 6: G loss: 0.1703691610268184 vs. C loss: 0.18946770909759733\n",
      "Group 11, Epoch 1: G loss: 0.24248327229704175 vs. C loss: 0.19730711976687113\n",
      "Group 11, Epoch 2: G loss: 0.2007127331835883 vs. C loss: 0.15084026919470891\n",
      "Group 11, Epoch 3: G loss: 0.2862460217305592 vs. C loss: 0.16344152473741108\n",
      "Group 11, Epoch 4: G loss: 0.4243941605091095 vs. C loss: 0.08833337326844533\n",
      "Group 11, Epoch 5: G loss: 0.5388528909002032 vs. C loss: 0.10775887303882174\n",
      "Group 11, Epoch 6: G loss: 0.4782292885439737 vs. C loss: 0.21001197397708893\n",
      "Group 12, Epoch 1: G loss: 0.18151989345039637 vs. C loss: 0.2158715824286143\n",
      "Group 12, Epoch 2: G loss: 0.10068081340619496 vs. C loss: 0.1796701616711087\n",
      "Group 12, Epoch 3: G loss: 0.14701060099261148 vs. C loss: 0.15339973072210947\n",
      "Group 12, Epoch 4: G loss: 0.3212546548673085 vs. C loss: 0.15766763852702245\n",
      "Group 12, Epoch 5: G loss: 0.26290593573025295 vs. C loss: 0.16745584458112717\n",
      "Group 12, Epoch 6: G loss: 0.3796659767627716 vs. C loss: 0.10074631538656022\n",
      "Group 12, Epoch 7: G loss: 0.5250659567969185 vs. C loss: 0.13445996161964205\n",
      "Group 12, Epoch 8: G loss: 0.4560238514627729 vs. C loss: 0.10172450045744579\n",
      "Group 12, Epoch 9: G loss: 0.4196111031941005 vs. C loss: 0.12504130519098705\n",
      "Group 13, Epoch 1: G loss: 0.18754840280328477 vs. C loss: 0.2205443216694726\n",
      "Group 13, Epoch 2: G loss: 0.14050471378224236 vs. C loss: 0.15665454831388262\n",
      "Group 13, Epoch 3: G loss: 0.36353077888488766 vs. C loss: 0.13075791216558882\n",
      "Group 13, Epoch 4: G loss: 0.3354126432112285 vs. C loss: 0.17430209699604246\n",
      "Group 13, Epoch 5: G loss: 0.1561786479183606 vs. C loss: 0.1760547450847096\n",
      "Group 13, Epoch 6: G loss: 0.15054082136069025 vs. C loss: 0.21308544443713295\n",
      "Group 14, Epoch 1: G loss: 0.2410234332084656 vs. C loss: 0.20197578436798522\n",
      "Group 14, Epoch 2: G loss: 0.15288112078394206 vs. C loss: 0.18661106791761184\n",
      "Group 14, Epoch 3: G loss: 0.1547190955707005 vs. C loss: 0.14402168989181519\n",
      "Group 14, Epoch 4: G loss: 0.36143787332943506 vs. C loss: 0.09740845113992691\n",
      "Group 14, Epoch 5: G loss: 0.5561619119984763 vs. C loss: 0.11942213194237815\n",
      "Group 14, Epoch 6: G loss: 0.5324500665068627 vs. C loss: 0.19928361972173056\n",
      "Group 15, Epoch 1: G loss: 0.18008462978260859 vs. C loss: 0.21155594123734367\n",
      "Group 15, Epoch 2: G loss: 0.17943709705557143 vs. C loss: 0.1481933444738388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 15, Epoch 3: G loss: 0.5497164283479963 vs. C loss: 0.15634434007936052\n",
      "Group 15, Epoch 4: G loss: 0.1842292755842209 vs. C loss: 0.19302926874823043\n",
      "Group 15, Epoch 5: G loss: 0.06047284251877239 vs. C loss: 0.2004453515013059\n",
      "Group 15, Epoch 6: G loss: 0.04324741576399122 vs. C loss: 0.2039802240000831\n",
      "Group 16, Epoch 1: G loss: 0.238893746478217 vs. C loss: 0.20387256062693063\n",
      "Group 16, Epoch 2: G loss: 0.2107290208339691 vs. C loss: 0.15724354485670725\n",
      "Group 16, Epoch 3: G loss: 0.32568640283175876 vs. C loss: 0.10602167910999723\n",
      "Group 16, Epoch 4: G loss: 0.6293088776724679 vs. C loss: 0.10925170282522838\n",
      "Group 16, Epoch 5: G loss: 0.5211867800780705 vs. C loss: 0.1042828435699145\n",
      "Group 16, Epoch 6: G loss: 0.6178971929209573 vs. C loss: 0.12959883444839052\n",
      "Group 16, Epoch 7: G loss: 0.3488809952778476 vs. C loss: 0.22236363838116327\n",
      "Group 17, Epoch 1: G loss: 0.21647691258362362 vs. C loss: 0.18734682185782325\n",
      "Group 17, Epoch 2: G loss: 0.14413553859506334 vs. C loss: 0.17076800184117424\n",
      "Group 17, Epoch 3: G loss: 0.21864482504980903 vs. C loss: 0.1321953982114792\n",
      "Group 17, Epoch 4: G loss: 0.37945832950728275 vs. C loss: 0.13758492925100857\n",
      "Group 17, Epoch 5: G loss: 0.3203720216240202 vs. C loss: 0.11068428970045513\n",
      "Group 17, Epoch 6: G loss: 0.7297326309340341 vs. C loss: 0.023467369584573638\n",
      "Group 17, Epoch 7: G loss: 0.7206230657441276 vs. C loss: 0.018897796153194375\n",
      "Group 18, Epoch 1: G loss: 0.15661734406437192 vs. C loss: 0.21685932328303656\n",
      "Group 18, Epoch 2: G loss: 0.11173162673200879 vs. C loss: 0.1798120869530572\n",
      "Group 18, Epoch 3: G loss: 0.13107453669820515 vs. C loss: 0.14095630413956114\n",
      "Group 18, Epoch 4: G loss: 0.44844672850200107 vs. C loss: 0.15051621281438402\n",
      "Group 18, Epoch 5: G loss: 0.26643155430044446 vs. C loss: 0.14548570662736895\n",
      "Group 18, Epoch 6: G loss: 0.3543725073337555 vs. C loss: 0.11549272802140977\n",
      "Group 18, Epoch 7: G loss: 0.49753181934356683 vs. C loss: 0.12245804154210621\n",
      "Group 18, Epoch 8: G loss: 0.6965629764965602 vs. C loss: 0.037150842861996755\n",
      "Group 18, Epoch 9: G loss: 0.5875180602073671 vs. C loss: 0.03829784277412627\n",
      "Group 19, Epoch 1: G loss: 0.21292837389877867 vs. C loss: 0.20301792522271475\n",
      "Group 19, Epoch 2: G loss: 0.1396420764071601 vs. C loss: 0.17237600021892122\n",
      "Group 19, Epoch 3: G loss: 0.31397661566734314 vs. C loss: 0.11282882425520156\n",
      "Group 19, Epoch 4: G loss: 0.5211818371500287 vs. C loss: 0.08332837269537978\n",
      "Group 19, Epoch 5: G loss: 0.871882574898856 vs. C loss: 0.018533361765245598\n",
      "Group 19, Epoch 6: G loss: 0.7361059938158306 vs. C loss: 0.01657931943837967\n",
      "Group 20, Epoch 1: G loss: 0.18034027112381798 vs. C loss: 0.21208164261447057\n",
      "Group 20, Epoch 2: G loss: 0.1390948819262641 vs. C loss: 0.1645769410663181\n",
      "Group 20, Epoch 3: G loss: 0.2410150034087045 vs. C loss: 0.20052223569817015\n",
      "Group 20, Epoch 4: G loss: 0.14226826493229183 vs. C loss: 0.17460342082712385\n",
      "Group 20, Epoch 5: G loss: 0.19682448974677494 vs. C loss: 0.17460417416360643\n",
      "Group 20, Epoch 6: G loss: 0.32334039828607014 vs. C loss: 0.22016559375656974\n",
      "Group 21, Epoch 1: G loss: 0.217392606820379 vs. C loss: 0.2053619407945209\n",
      "Group 21, Epoch 2: G loss: 0.17085323546613965 vs. C loss: 0.15917232715421256\n",
      "Group 21, Epoch 3: G loss: 0.24182705070291247 vs. C loss: 0.1279352866113186\n",
      "Group 21, Epoch 4: G loss: 0.6075767363820758 vs. C loss: 0.10097129229042266\n",
      "Group 21, Epoch 5: G loss: 0.4223805866071157 vs. C loss: 0.132450165020095\n",
      "Group 21, Epoch 6: G loss: 0.4590981538806642 vs. C loss: 0.22910629709561667\n",
      "Group 22, Epoch 1: G loss: 0.14465842374733517 vs. C loss: 0.22318956918186614\n",
      "Group 22, Epoch 2: G loss: 0.058496845939329696 vs. C loss: 0.19265847073660958\n",
      "Group 22, Epoch 3: G loss: 0.061411137453147344 vs. C loss: 0.17563738591141168\n",
      "Group 22, Epoch 4: G loss: 0.18873848638364246 vs. C loss: 0.16198021835751003\n",
      "Group 22, Epoch 5: G loss: 0.4188570703778948 vs. C loss: 0.12558061215612623\n",
      "Group 22, Epoch 6: G loss: 0.30138137255396164 vs. C loss: 0.09804523156748877\n",
      "Group 22, Epoch 7: G loss: 0.6302598978791919 vs. C loss: 0.07056346866819593\n",
      "Group 22, Epoch 8: G loss: 0.7355994582176208 vs. C loss: 0.04567245187030899\n",
      "Group 23, Epoch 1: G loss: 0.19838802835771016 vs. C loss: 0.20953575107786393\n",
      "Group 23, Epoch 2: G loss: 0.14570800662040712 vs. C loss: 0.16873847775989106\n",
      "Group 23, Epoch 3: G loss: 0.3025564704622541 vs. C loss: 0.14439889126353794\n",
      "Group 23, Epoch 4: G loss: 0.37768664700644355 vs. C loss: 0.16295327742894491\n",
      "Group 23, Epoch 5: G loss: 0.35112140945025855 vs. C loss: 0.11565999976462787\n",
      "Group 23, Epoch 6: G loss: 0.5312448390892573 vs. C loss: 0.07116372407310538\n",
      "Group 24, Epoch 1: G loss: 0.24436936719076977 vs. C loss: 0.19982307735416624\n",
      "Group 24, Epoch 2: G loss: 0.16798829308577945 vs. C loss: 0.1603900964061419\n",
      "Group 24, Epoch 3: G loss: 0.3035009801387787 vs. C loss: 0.1255595866176817\n",
      "Group 24, Epoch 4: G loss: 0.46982296449797495 vs. C loss: 0.12240553109182252\n",
      "Group 24, Epoch 5: G loss: 0.2912515103816986 vs. C loss: 0.18505545622772637\n",
      "Group 24, Epoch 6: G loss: 0.13625122095857348 vs. C loss: 0.1750850114557478\n",
      "Group 25, Epoch 1: G loss: 0.23122171504156927 vs. C loss: 0.20411605305141875\n",
      "Group 25, Epoch 2: G loss: 0.2624707422086171 vs. C loss: 0.13410070869657728\n",
      "Group 25, Epoch 3: G loss: 0.4184593847819737 vs. C loss: 0.10582006081110902\n",
      "Group 25, Epoch 4: G loss: 0.745009081704276 vs. C loss: 0.0528782754101687\n",
      "Group 25, Epoch 5: G loss: 0.3453241209898676 vs. C loss: 0.12473204876813622\n",
      "Group 25, Epoch 6: G loss: 0.6176182591489384 vs. C loss: 0.21867367376883826\n",
      "Group 26, Epoch 1: G loss: 0.18490523525646757 vs. C loss: 0.21363350749015808\n",
      "Group 26, Epoch 2: G loss: 0.11487883840288435 vs. C loss: 0.17727737377087274\n",
      "Group 26, Epoch 3: G loss: 0.15245003146784647 vs. C loss: 0.15166368914975062\n",
      "Group 26, Epoch 4: G loss: 0.4364940387862069 vs. C loss: 0.11014927923679352\n",
      "Group 26, Epoch 5: G loss: 0.46408531538077763 vs. C loss: 0.16086526463429132\n",
      "Group 26, Epoch 6: G loss: 0.2021461505975042 vs. C loss: 0.1294895402259297\n",
      "Group 27, Epoch 1: G loss: 0.18788745360715048 vs. C loss: 0.2218422277106179\n",
      "Group 27, Epoch 2: G loss: 0.09350700548716954 vs. C loss: 0.19448378847704992\n",
      "Group 27, Epoch 3: G loss: 0.10882745321307863 vs. C loss: 0.18295399016804167\n",
      "Group 27, Epoch 4: G loss: 0.1824030420609883 vs. C loss: 0.1465147924092081\n",
      "Group 27, Epoch 5: G loss: 0.442240275655474 vs. C loss: 0.13476005031002894\n",
      "Group 27, Epoch 6: G loss: 0.29573069896016807 vs. C loss: 0.16507707784573236\n",
      "Group 27, Epoch 7: G loss: 0.3465704330376216 vs. C loss: 0.15182137986024222\n",
      "Group 27, Epoch 8: G loss: 0.5701555354254586 vs. C loss: 0.06959107766548793\n",
      "Group 28, Epoch 1: G loss: 0.15368799950395312 vs. C loss: 0.22050490230321884\n",
      "Group 28, Epoch 2: G loss: 0.10038828487907137 vs. C loss: 0.1779757804340786\n",
      "Group 28, Epoch 3: G loss: 0.13668889807803292 vs. C loss: 0.151204087667995\n",
      "Group 28, Epoch 4: G loss: 0.3003680131265095 vs. C loss: 0.17225237521860334\n",
      "Group 28, Epoch 5: G loss: 0.23192237274987357 vs. C loss: 0.19001662731170654\n",
      "Group 28, Epoch 6: G loss: 0.10744122841528483 vs. C loss: 0.1822967943218019\n",
      "Group 29, Epoch 1: G loss: 0.17000648805073332 vs. C loss: 0.2208421809805764\n",
      "Group 29, Epoch 2: G loss: 0.12021699462618145 vs. C loss: 0.16345356487565568\n",
      "Group 29, Epoch 3: G loss: 0.24757327948297772 vs. C loss: 0.12560141914420658\n",
      "Group 29, Epoch 4: G loss: 0.46999984894480024 vs. C loss: 0.12913469142384\n",
      "Group 29, Epoch 5: G loss: 0.3921536351953233 vs. C loss: 0.12061013198561138\n",
      "Group 29, Epoch 6: G loss: 0.6742312158857073 vs. C loss: 0.05168739582101504\n",
      "Group 29, Epoch 7: G loss: 0.7198417552879878 vs. C loss: 0.13873458819256887\n",
      "Group 30, Epoch 1: G loss: 0.1852944878595216 vs. C loss: 0.21366388102372488\n",
      "Group 30, Epoch 2: G loss: 0.13471337173666273 vs. C loss: 0.16305001990662682\n",
      "Group 30, Epoch 3: G loss: 0.2361002636807306 vs. C loss: 0.12760302093293932\n",
      "Group 30, Epoch 4: G loss: 0.46116949064391 vs. C loss: 0.09742734870976871\n",
      "Group 30, Epoch 5: G loss: 0.39834170937538144 vs. C loss: 0.1467725224792957\n",
      "Group 30, Epoch 6: G loss: 0.3826627377952848 vs. C loss: 0.14069156928194892\n",
      "Group 30, Epoch 7: G loss: 0.657534692968641 vs. C loss: 0.09092663476864497\n",
      "Group 31, Epoch 1: G loss: 0.18913378545216153 vs. C loss: 0.213510329524676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 31, Epoch 2: G loss: 0.1062216252088547 vs. C loss: 0.17660202251540288\n",
      "Group 31, Epoch 3: G loss: 0.1975490940468652 vs. C loss: 0.15413266917069754\n",
      "Group 31, Epoch 4: G loss: 0.4026832329375403 vs. C loss: 0.1713985121912426\n",
      "Group 31, Epoch 5: G loss: 0.21851871481963564 vs. C loss: 0.1586487665772438\n",
      "Group 31, Epoch 6: G loss: 0.3374264236007419 vs. C loss: 0.13350022004710302\n",
      "Group 32, Epoch 1: G loss: 0.21650035423891884 vs. C loss: 0.20940686927901375\n",
      "Group 32, Epoch 2: G loss: 0.15197775491646356 vs. C loss: 0.1663158006138272\n",
      "Group 32, Epoch 3: G loss: 0.29291155806609565 vs. C loss: 0.15010456575287714\n",
      "Group 32, Epoch 4: G loss: 0.527997989313943 vs. C loss: 0.0940915880103906\n",
      "Group 32, Epoch 5: G loss: 0.388155020560537 vs. C loss: 0.11248677720626195\n",
      "Group 32, Epoch 6: G loss: 0.7118120125361852 vs. C loss: 0.040629552159872316\n",
      "Group 33, Epoch 1: G loss: 0.21014123537710735 vs. C loss: 0.212392744090822\n",
      "Group 33, Epoch 2: G loss: 0.08831049501895905 vs. C loss: 0.18606107599205438\n",
      "Group 33, Epoch 3: G loss: 0.08154003279549736 vs. C loss: 0.1596736146344079\n",
      "Group 33, Epoch 4: G loss: 0.3574618663106646 vs. C loss: 0.11385764429966609\n",
      "Group 33, Epoch 5: G loss: 0.5797257474490575 vs. C loss: 0.11712156981229783\n",
      "Group 33, Epoch 6: G loss: 0.6421430264200483 vs. C loss: 0.05189822241663933\n",
      "Group 33, Epoch 7: G loss: 0.43675428330898286 vs. C loss: 0.22154952254560256\n",
      "Group 34, Epoch 1: G loss: 0.21897056954247612 vs. C loss: 0.19412521355681953\n",
      "Group 34, Epoch 2: G loss: 0.18168467836720606 vs. C loss: 0.16086667941676247\n",
      "Group 34, Epoch 3: G loss: 0.1975408392293113 vs. C loss: 0.14646381802029082\n",
      "Group 34, Epoch 4: G loss: 0.5165771629129138 vs. C loss: 0.0845153128935231\n",
      "Group 34, Epoch 5: G loss: 0.5248033804552895 vs. C loss: 0.09727413290076786\n",
      "Group 34, Epoch 6: G loss: 0.31470604028020593 vs. C loss: 0.21842694034179053\n",
      "Group 35, Epoch 1: G loss: 0.18687854664666312 vs. C loss: 0.21051729056570265\n",
      "Group 35, Epoch 2: G loss: 0.10991514857326234 vs. C loss: 0.18093273043632507\n",
      "Group 35, Epoch 3: G loss: 0.11045460679701395 vs. C loss: 0.16511227356062994\n",
      "Group 35, Epoch 4: G loss: 0.2997613063880376 vs. C loss: 0.12804577747980753\n",
      "Group 35, Epoch 5: G loss: 0.596449077129364 vs. C loss: 0.08272203430533409\n",
      "Group 35, Epoch 6: G loss: 0.4901119023561477 vs. C loss: 0.18933778173393676\n",
      "Group 36, Epoch 1: G loss: 0.19754712922232492 vs. C loss: 0.20272317197587755\n",
      "Group 36, Epoch 2: G loss: 0.15679983794689178 vs. C loss: 0.16524303787284425\n",
      "Group 36, Epoch 3: G loss: 0.31845609290259225 vs. C loss: 0.11894181784656314\n",
      "Group 36, Epoch 4: G loss: 0.42712599720273703 vs. C loss: 0.19280159018105927\n",
      "Group 36, Epoch 5: G loss: 0.09938102619988579 vs. C loss: 0.20038711527983347\n",
      "Group 36, Epoch 6: G loss: 0.10153372809290886 vs. C loss: 0.19322664787371954\n",
      "Group 37, Epoch 1: G loss: 0.22049616106918882 vs. C loss: 0.21369222468800017\n",
      "Group 37, Epoch 2: G loss: 0.20430158036095758 vs. C loss: 0.1263723663157887\n",
      "Group 37, Epoch 3: G loss: 0.5878618121147156 vs. C loss: 0.08463331974214977\n",
      "Group 37, Epoch 4: G loss: 0.40558949368340624 vs. C loss: 0.12467308218280475\n",
      "Group 37, Epoch 5: G loss: 0.36063476566757474 vs. C loss: 0.22299783925215402\n",
      "Group 37, Epoch 6: G loss: 0.08613040787833078 vs. C loss: 0.13963262322876188\n",
      "Group 38, Epoch 1: G loss: 0.2399879915373666 vs. C loss: 0.17279340906275645\n",
      "Group 38, Epoch 2: G loss: 0.25157150413308826 vs. C loss: 0.20589284847180048\n",
      "Group 38, Epoch 3: G loss: 0.04945914564388138 vs. C loss: 0.1920682460897499\n",
      "Group 38, Epoch 4: G loss: 0.061917274551732196 vs. C loss: 0.17329804102579752\n",
      "Group 38, Epoch 5: G loss: 0.27306260594299864 vs. C loss: 0.14810382160875532\n",
      "Group 38, Epoch 6: G loss: 0.4010838116918291 vs. C loss: 0.11519403590096366\n",
      "Group 39, Epoch 1: G loss: 0.19270816104752678 vs. C loss: 0.20864942835436925\n",
      "Group 39, Epoch 2: G loss: 0.1393403593982969 vs. C loss: 0.1742784372634358\n",
      "Group 39, Epoch 3: G loss: 0.1933551916054317 vs. C loss: 0.14464684989717272\n",
      "Group 39, Epoch 4: G loss: 0.49908384425299507 vs. C loss: 0.12539132684469223\n",
      "Group 39, Epoch 5: G loss: 0.3049325153231622 vs. C loss: 0.19130504876375198\n",
      "Group 39, Epoch 6: G loss: 0.16205903483288628 vs. C loss: 0.11199323253499138\n",
      "Group 40, Epoch 1: G loss: 0.26352223115307943 vs. C loss: 0.18993869589434728\n",
      "Group 40, Epoch 2: G loss: 0.2290423116513661 vs. C loss: 0.1771310716867447\n",
      "Group 40, Epoch 3: G loss: 0.3211960673332214 vs. C loss: 0.12166473600599502\n",
      "Group 40, Epoch 4: G loss: 0.413650073323931 vs. C loss: 0.1546143376164966\n",
      "Group 40, Epoch 5: G loss: 0.3459271775824683 vs. C loss: 0.13805599924590853\n",
      "Group 40, Epoch 6: G loss: 0.5648475595882961 vs. C loss: 0.07948117123709784\n",
      "Group 41, Epoch 1: G loss: 0.2346065797976085 vs. C loss: 0.1898024082183838\n",
      "Group 41, Epoch 2: G loss: 0.2143374264240265 vs. C loss: 0.1483074294196235\n",
      "Group 41, Epoch 3: G loss: 0.44306614909853254 vs. C loss: 0.11074730008840561\n",
      "Group 41, Epoch 4: G loss: 0.47454027874129157 vs. C loss: 0.07036220406492551\n",
      "Group 41, Epoch 5: G loss: 0.6460209846496582 vs. C loss: 0.17289326753881243\n",
      "Group 41, Epoch 6: G loss: 0.21969527139195372 vs. C loss: 0.22704898317654928\n",
      "Group 42, Epoch 1: G loss: 0.2190423650400979 vs. C loss: 0.19558765035536554\n",
      "Group 42, Epoch 2: G loss: 0.1415793782898358 vs. C loss: 0.18035587088929284\n",
      "Group 42, Epoch 3: G loss: 0.10375654974154062 vs. C loss: 0.1615249663591385\n",
      "Group 42, Epoch 4: G loss: 0.30707599009786335 vs. C loss: 0.14118320991595587\n",
      "Group 42, Epoch 5: G loss: 0.31497090714318404 vs. C loss: 0.11010533819595973\n",
      "Group 42, Epoch 6: G loss: 0.4326592453888484 vs. C loss: 0.13098014891147614\n",
      "Group 43, Epoch 1: G loss: 0.24055308018411908 vs. C loss: 0.21228527029355368\n",
      "Group 43, Epoch 2: G loss: 0.18643324758325305 vs. C loss: 0.1452051591542032\n",
      "Group 43, Epoch 3: G loss: 0.404959990297045 vs. C loss: 0.09326136484742165\n",
      "Group 43, Epoch 4: G loss: 0.561599177122116 vs. C loss: 0.05098258683251009\n",
      "Group 43, Epoch 5: G loss: 0.4746393995625631 vs. C loss: 0.2081625229782528\n",
      "Group 43, Epoch 6: G loss: 0.02462072271321501 vs. C loss: 0.20858487031526038\n",
      "Group 44, Epoch 1: G loss: 0.1960061307464327 vs. C loss: 0.19818132287926146\n",
      "Group 44, Epoch 2: G loss: 0.18785126166684288 vs. C loss: 0.15324216418796116\n",
      "Group 44, Epoch 3: G loss: 0.27290436902216497 vs. C loss: 0.21178367733955383\n",
      "Group 44, Epoch 4: G loss: 0.08630868888327055 vs. C loss: 0.20466933730575776\n",
      "Group 44, Epoch 5: G loss: 0.041285762935876845 vs. C loss: 0.1959594815141625\n",
      "Group 44, Epoch 6: G loss: 0.0622676422553403 vs. C loss: 0.17327702376577592\n",
      "Group 45, Epoch 1: G loss: 0.2337745155606951 vs. C loss: 0.18574146760834587\n",
      "Group 45, Epoch 2: G loss: 0.34042831489018033 vs. C loss: 0.1273762931426366\n",
      "Group 45, Epoch 3: G loss: 0.4802973892007555 vs. C loss: 0.1244041845202446\n",
      "Group 45, Epoch 4: G loss: 0.5675655347960336 vs. C loss: 0.036675736411578126\n",
      "Group 45, Epoch 5: G loss: 0.7972668460437229 vs. C loss: 0.08290432310766643\n",
      "Group 45, Epoch 6: G loss: 0.5463101301874432 vs. C loss: 0.21791129476494261\n",
      "Group 46, Epoch 1: G loss: 0.20144079795905517 vs. C loss: 0.2203225725226932\n",
      "Group 46, Epoch 2: G loss: 0.09415282926389149 vs. C loss: 0.18764614396625098\n",
      "Group 46, Epoch 3: G loss: 0.10747715894665037 vs. C loss: 0.16080842581060198\n",
      "Group 46, Epoch 4: G loss: 0.2976004413196019 vs. C loss: 0.1358092510037952\n",
      "Group 46, Epoch 5: G loss: 0.41497962559972484 vs. C loss: 0.08007843751046392\n",
      "Group 46, Epoch 6: G loss: 0.6063552243368967 vs. C loss: 0.06974939670827653\n",
      "Group 47, Epoch 1: G loss: 0.21886886613709586 vs. C loss: 0.19810799260934195\n",
      "Group 47, Epoch 2: G loss: 0.2257769593170711 vs. C loss: 0.14465229709943137\n",
      "Group 47, Epoch 3: G loss: 0.5475983807018825 vs. C loss: 0.08072423769368066\n",
      "Group 47, Epoch 4: G loss: 0.6005960839135306 vs. C loss: 0.07395860014690293\n",
      "Group 47, Epoch 5: G loss: 0.8428461449486868 vs. C loss: 0.06954519699017207\n",
      "Group 47, Epoch 6: G loss: 0.30033208451100757 vs. C loss: 0.21391351355446708\n",
      "Group 47, Epoch 7: G loss: 0.04629854357668332 vs. C loss: 0.20779170882370734\n",
      "Group 48, Epoch 1: G loss: 0.19734178824084148 vs. C loss: 0.2029216504759259\n",
      "Group 48, Epoch 2: G loss: 0.17803109245640888 vs. C loss: 0.17652371608548692\n",
      "Group 48, Epoch 3: G loss: 0.14877411936010634 vs. C loss: 0.1384162041876051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 48, Epoch 4: G loss: 0.3444986249719348 vs. C loss: 0.1521239620116022\n",
      "Group 48, Epoch 5: G loss: 0.3151379759822573 vs. C loss: 0.18882716364330718\n",
      "Group 48, Epoch 6: G loss: 0.09379624639238629 vs. C loss: 0.19428702526622346\n",
      "Group 49, Epoch 1: G loss: 0.21358138578278676 vs. C loss: 0.2151262081331677\n",
      "Group 49, Epoch 2: G loss: 0.16862297569002424 vs. C loss: 0.16649776490198243\n",
      "Group 49, Epoch 3: G loss: 0.22329703313963756 vs. C loss: 0.12706517428159714\n",
      "Group 49, Epoch 4: G loss: 0.5626971687589373 vs. C loss: 0.08833691436383458\n",
      "Group 49, Epoch 5: G loss: 0.43966812491416934 vs. C loss: 0.0763531687359015\n",
      "Group 49, Epoch 6: G loss: 0.7874181764466422 vs. C loss: 0.023825433519151475\n",
      "Group 50, Epoch 1: G loss: 0.21138928277151922 vs. C loss: 0.2042668213446935\n",
      "Group 50, Epoch 2: G loss: 0.15060761528355734 vs. C loss: 0.16875112387869093\n",
      "Group 50, Epoch 3: G loss: 0.19040412945406776 vs. C loss: 0.1951480019423697\n",
      "Group 50, Epoch 4: G loss: 0.15433205855744228 vs. C loss: 0.19707507722907594\n",
      "Group 50, Epoch 5: G loss: 0.07521702040519032 vs. C loss: 0.19668262203534445\n",
      "Group 50, Epoch 6: G loss: 0.08816546116556442 vs. C loss: 0.17799747983614603\n",
      "Group 51, Epoch 1: G loss: 0.20369732763086043 vs. C loss: 0.20870385277602407\n",
      "Group 51, Epoch 2: G loss: 0.19794762986046927 vs. C loss: 0.14884144067764282\n",
      "Group 51, Epoch 3: G loss: 0.4592076701777322 vs. C loss: 0.12072910534010993\n",
      "Group 51, Epoch 4: G loss: 0.25686624795198437 vs. C loss: 0.19439614729748833\n",
      "Group 51, Epoch 5: G loss: 0.054031792283058155 vs. C loss: 0.2022020055188073\n",
      "Group 51, Epoch 6: G loss: 0.024180474185517855 vs. C loss: 0.1998694009251065\n",
      "Group 52, Epoch 1: G loss: 0.22466860072953357 vs. C loss: 0.17487222204605737\n",
      "Group 52, Epoch 2: G loss: 0.3116875444139753 vs. C loss: 0.1173932891752985\n",
      "Group 52, Epoch 3: G loss: 0.5059476545878819 vs. C loss: 0.12762539585431418\n",
      "Group 52, Epoch 4: G loss: 0.7454816392489843 vs. C loss: 0.03593492963247829\n",
      "Group 52, Epoch 5: G loss: 0.4879577334438051 vs. C loss: 0.1988158408138487\n",
      "Group 52, Epoch 6: G loss: 0.03207725185368743 vs. C loss: 0.20977825505865944\n",
      "Group 53, Epoch 1: G loss: 0.19845018173967088 vs. C loss: 0.2137550993098153\n",
      "Group 53, Epoch 2: G loss: 0.15301683587687356 vs. C loss: 0.15720710075563857\n",
      "Group 53, Epoch 3: G loss: 0.3044543819768088 vs. C loss: 0.10067353811528947\n",
      "Group 53, Epoch 4: G loss: 0.6150917998382024 vs. C loss: 0.10137865237063833\n",
      "Group 53, Epoch 5: G loss: 0.5972679615020752 vs. C loss: 0.08239770597881742\n",
      "Group 53, Epoch 6: G loss: 0.28923200124076437 vs. C loss: 0.2137934184736676\n",
      "Group 54, Epoch 1: G loss: 0.14856654363019126 vs. C loss: 0.21683660646279654\n",
      "Group 54, Epoch 2: G loss: 0.1387871327144759 vs. C loss: 0.16280128641260994\n",
      "Group 54, Epoch 3: G loss: 0.1739150268690927 vs. C loss: 0.16229782005151114\n",
      "Group 54, Epoch 4: G loss: 0.46608451477118906 vs. C loss: 0.18754037138488558\n",
      "Group 54, Epoch 5: G loss: 0.19458970363651001 vs. C loss: 0.21055708991156685\n",
      "Group 54, Epoch 6: G loss: 0.04885012049760137 vs. C loss: 0.19969477007786432\n",
      "Group 55, Epoch 1: G loss: 0.20797906560557228 vs. C loss: 0.2163364738225937\n",
      "Group 55, Epoch 2: G loss: 0.19088418952056338 vs. C loss: 0.14633205864164564\n",
      "Group 55, Epoch 3: G loss: 0.28532484599522184 vs. C loss: 0.10820517357852723\n",
      "Group 55, Epoch 4: G loss: 0.6838137984275818 vs. C loss: 0.05162326453460587\n",
      "Group 55, Epoch 5: G loss: 0.683459530557905 vs. C loss: 0.0412742979824543\n",
      "Group 55, Epoch 6: G loss: 0.8430127620697021 vs. C loss: 0.03284224081370566\n",
      "Group 55, Epoch 7: G loss: 0.5341369100979396 vs. C loss: 0.20992789996994865\n",
      "Group 55, Epoch 8: G loss: 0.03232726124780519 vs. C loss: 0.20771331091721854\n",
      "Group 56, Epoch 1: G loss: 0.20300796478986743 vs. C loss: 0.215159239868323\n",
      "Group 56, Epoch 2: G loss: 0.10156550088099071 vs. C loss: 0.17470594578319124\n",
      "Group 56, Epoch 3: G loss: 0.18493421162877763 vs. C loss: 0.12909667525026533\n",
      "Group 56, Epoch 4: G loss: 0.46243316105433874 vs. C loss: 0.09685653986202346\n",
      "Group 56, Epoch 5: G loss: 0.18564141371420448 vs. C loss: 0.2053455412387848\n",
      "Group 56, Epoch 6: G loss: 0.03281849212944508 vs. C loss: 0.20500910613271925\n",
      "Group 57, Epoch 1: G loss: 0.18365456547055925 vs. C loss: 0.20828711489836374\n",
      "Group 57, Epoch 2: G loss: 0.20798021384647916 vs. C loss: 0.13735243181387582\n",
      "Group 57, Epoch 3: G loss: 0.41402993883405415 vs. C loss: 0.12002662320931752\n",
      "Group 57, Epoch 4: G loss: 0.5061852604150772 vs. C loss: 0.207754307323032\n",
      "Group 57, Epoch 5: G loss: 0.032323070509093145 vs. C loss: 0.20611438734663856\n",
      "Group 57, Epoch 6: G loss: 0.014214419972683703 vs. C loss: 0.20610275864601135\n",
      "Group 58, Epoch 1: G loss: 0.22219894145216262 vs. C loss: 0.20006337430742052\n",
      "Group 58, Epoch 2: G loss: 0.17611678881304602 vs. C loss: 0.16342862612671324\n",
      "Group 58, Epoch 3: G loss: 0.2482939741441182 vs. C loss: 0.10897178534004424\n",
      "Group 58, Epoch 4: G loss: 0.6318868517875672 vs. C loss: 0.07450606922308603\n",
      "Group 58, Epoch 5: G loss: 0.5593201356274741 vs. C loss: 0.09105628977219264\n",
      "Group 58, Epoch 6: G loss: 0.427719336482031 vs. C loss: 0.21960849894417656\n",
      "Group 59, Epoch 1: G loss: 0.21419700128691538 vs. C loss: 0.20728200674057007\n",
      "Group 59, Epoch 2: G loss: 0.13635048483099255 vs. C loss: 0.17046296348174414\n",
      "Group 59, Epoch 3: G loss: 0.16794245775256836 vs. C loss: 0.1860116041368908\n",
      "Group 59, Epoch 4: G loss: 0.31481247799737105 vs. C loss: 0.14391101234489015\n",
      "Group 59, Epoch 5: G loss: 0.3749298176595142 vs. C loss: 0.12973416348298392\n",
      "Group 59, Epoch 6: G loss: 0.4054314677204404 vs. C loss: 0.20183282428317603\n"
     ]
    }
   ],
   "source": [
    "loss_ma = [90, 90, 90];\n",
    "extvar = {\"begin\": 10};\n",
    "\n",
    "def plot_current_map(inputs):\n",
    "    # plot it each epoch\n",
    "    mp = construct_map_with_sliders(inputs, extvar=extvar);\n",
    "    # to make it clearer, add the start pos\n",
    "    npa = np.concatenate([[np.concatenate([extvar[\"start_pos\"] / np.array([512, 384]), [0, 0]])], tf.stack(mp).numpy().squeeze()])\n",
    "    fig, ax = plt.subplots()\n",
    "    x, y = np.transpose(npa)[0:2]\n",
    "    #x, y = np.random.rand(2, 20)\n",
    "    line = MyLine(x, y, mfc='red', ms=12)\n",
    "    line.text.set_color('red')\n",
    "    line.text.set_fontsize(16)\n",
    "    ax.add_line(line)\n",
    "    plt.show()\n",
    "\n",
    "def generative_model(in_params, out_params, loss_func='mse'):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(in_params,)),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(128, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(out_params, activation=tf.nn.tanh)#,\n",
    "#         keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(out_params,))\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        optimizer = tf.optimizers.Adam(0.002) #Adamoptimizer?\n",
    "    except:\n",
    "        optimizer = tf.train.AdamOptimizer(0.002) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss=loss_func,\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n",
    "\n",
    "def mixed_model(generator, mapping_layer, discriminator, in_params):\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    inp = keras.layers.Input(shape=(in_params,))\n",
    "    start_pos = keras.layers.Input(shape = (2,))#tf.convert_to_tensor([0, 0], dtype=tf.float32)\n",
    "    rel = keras.layers.Input(shape = (7, note_group_size))#tf.zeros((5, note_group_size), dtype=tf.float32)\n",
    "    interm1 = generator(inp)\n",
    "    interm2 = mapping_layer(interm1)\n",
    "    end = discriminator(interm2)\n",
    "    model = keras.Model(inputs = inp, outputs = [interm1, interm2, end])\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "\n",
    "    try:\n",
    "        optimizer = tf.optimizers.Adam(0.001) #Adamoptimizer?\n",
    "    except:\n",
    "        optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "        \n",
    "    losses = [AlwaysZeroCustomLoss(), BoxCustomLoss(), GenerativeCustomLoss()];\n",
    "\n",
    "    model.compile(loss=losses,\n",
    "                  loss_weights=[1e-8, 1, 1],\n",
    "                optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def conv_input(inp, extvar):\n",
    "#     Now it only uses single input\n",
    "    return inp;\n",
    "\n",
    "\n",
    "plot_noise = np.random.random((1, GAN_PARAMS[\"g_input_size\"]));\n",
    "\n",
    "# Pre-fit classifier for 1 epoch\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "# build models first, then train (it is faster in TF 2.0)\n",
    "\n",
    "def make_models():\n",
    "        \n",
    "    extvar[\"begin\"] = 0;\n",
    "    extvar[\"start_pos\"] = [256, 192];\n",
    "    extvar[\"length_multiplier\"] = 1;\n",
    "    extvar[\"next_from_slider_end\"] = GAN_PARAMS[\"next_from_slider_end\"];\n",
    "    \n",
    "    classifier_model = build_classifier_model();\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    g_input_size = GAN_PARAMS[\"g_input_size\"];\n",
    "    \n",
    "    gmodel = generative_model(g_input_size, note_group_size * 4);\n",
    "    mapping_layer = KerasCustomMappingLayer(extvar);\n",
    "    mmodel = mixed_model(gmodel, mapping_layer, classifier_model, g_input_size);\n",
    "    \n",
    "    default_weights = mmodel.get_weights();\n",
    "    \n",
    "    return gmodel, mapping_layer, classifier_model, mmodel, default_weights;\n",
    "\n",
    "def set_extvar(models, extvar):\n",
    "    gmodel, mapping_layer, classifier_model, mmodel, default_weights = models;\n",
    "    mapping_layer.set_extvar(extvar);\n",
    "    \n",
    "def reset_model_weights(models):\n",
    "    gmodel, mapping_layer, classifier_model, mmodel, default_weights = models;\n",
    "    weights = default_weights;\n",
    "    mmodel.set_weights(weights);\n",
    "\n",
    "# we can train all the classifiers first, onto Epoch X [x = 1~10]\n",
    "# then train the generators to fit to them\n",
    "# to reduce some training time.\n",
    "# but i think it doesn't work too well since it's the generator which is slow...\n",
    "\n",
    "def generate_set(models, begin = 0, start_pos=[256, 192], group_id=-1, length_multiplier=1, plot_map=True):\n",
    "    extvar[\"begin\"] = begin;\n",
    "    extvar[\"start_pos\"] = start_pos;\n",
    "    extvar[\"length_multiplier\"] = length_multiplier;\n",
    "    extvar[\"next_from_slider_end\"] = GAN_PARAMS[\"next_from_slider_end\"];\n",
    "    \n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    max_epoch = GAN_PARAMS[\"max_epoch\"];\n",
    "    good_epoch = GAN_PARAMS[\"good_epoch\"] - 1;\n",
    "    g_multiplier = GAN_PARAMS[\"g_epochs\"];\n",
    "    c_multiplier = GAN_PARAMS[\"c_epochs\"];\n",
    "    g_batch = GAN_PARAMS[\"g_batch\"];\n",
    "    g_input_size = GAN_PARAMS[\"g_input_size\"];\n",
    "    c_true_batch = GAN_PARAMS[\"c_true_batch\"];\n",
    "    c_false_batch = GAN_PARAMS[\"c_false_batch\"];\n",
    "    \n",
    "    reset_model_weights(models);\n",
    "    set_extvar(models, extvar);\n",
    "    gmodel, mapping_layer, classifier_model, mmodel, default_weights = models;\n",
    "    \n",
    "    # see the summaries\n",
    "#     gmodel.summary()\n",
    "#     classifier_model.summary()\n",
    "#     mmodel.summary()\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        \n",
    "        gnoise = np.random.random((g_batch, g_input_size));\n",
    "        glabel = [np.zeros((g_batch, note_group_size * 4)), np.ones((g_batch,)), np.ones((g_batch,))]\n",
    "        ginput = conv_input(gnoise, extvar);\n",
    "        \n",
    "        # fit mmodel instead of gmodel\n",
    "        history = mmodel.fit(ginput, glabel, epochs=g_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[])\n",
    "        \n",
    "        pred_noise = np.random.random((c_false_batch, g_input_size));\n",
    "        pred_input = conv_input(pred_noise, extvar);\n",
    "        predicted_maps_data, predicted_maps_mapped, _predclass = mmodel.predict(pred_input);\n",
    "        new_false_maps = predicted_maps_mapped;\n",
    "        new_false_labels = np.zeros(c_false_batch);\n",
    "        \n",
    "\n",
    "        rn = np.random.randint(0, special_train_data.shape[0], (c_true_batch,))\n",
    "        actual_train_data = np.concatenate((new_false_maps, special_train_data[rn]), axis=0); #special_false_data[st:se], \n",
    "        actual_train_labels = np.concatenate((new_false_labels, special_train_labels[rn]), axis=0); #special_false_labels[st:se], \n",
    "        \n",
    "    \n",
    "        history2 = classifier_model.fit(actual_train_data, actual_train_labels, epochs=c_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[])\n",
    "        \n",
    "        # calculate the losses\n",
    "        g_loss = np.mean(history.history['loss']);\n",
    "        c_loss = np.mean(history2.history['loss']);\n",
    "        print(\"Group {}, Epoch {}: G loss: {} vs. C loss: {}\".format(group_id, 1+i, g_loss, c_loss));\n",
    "        \n",
    "        # delete the history to free memory\n",
    "        del history, history2\n",
    "        \n",
    "        # make a new set of notes\n",
    "        res_noise = np.random.random((1, g_input_size));\n",
    "        res_input = conv_input(res_noise, extvar);\n",
    "        _resgenerated, res_map, _resclass = mmodel.predict(res_input);\n",
    "        if plot_map:\n",
    "            plot_current_map(tf.convert_to_tensor(res_map, dtype=tf.float32));\n",
    "        \n",
    "        # early return if found a good solution\n",
    "        # good is (inside the map boundary)\n",
    "        if i >= good_epoch:\n",
    "#             current_map = construct_map_with_sliders(tf.convert_to_tensor(res, dtype=\"float32\"), extvar=extvar);\n",
    "            current_map = res_map;\n",
    "            if inblock_trueness(current_map[:, :, 0:2]).numpy()[0] == 0 and inblock_trueness(current_map[:, :, 4:6]).numpy()[0] == 0:\n",
    "                # debugging options to check map integrity\n",
    "#                 print(tf.reduce_mean(current_map));\n",
    "#                 print(\"-----MAPLAYER-----\")\n",
    "#                 print(tf.reduce_mean(mapping_layer(conv_input(tf.convert_to_tensor(_resgenerated, dtype=\"float32\"), extvar))));\n",
    "#                 print(\"-----CMWS-----\")\n",
    "#                 print(tf.reduce_mean(construct_map_with_sliders(tf.convert_to_tensor(_resgenerated, dtype=\"float32\"), extvar=mapping_layer.extvar)));\n",
    "                break;\n",
    "\n",
    "#     plot_history(history);\n",
    "#     plot_history(history2);\n",
    "    if plot_map:\n",
    "        for i in range(3): # from our testing, any random input generates nearly the same map\n",
    "            plot_noise = np.random.random((1, g_input_size));\n",
    "            plot_input = conv_input(plot_noise, extvar);\n",
    "            _plotgenerated, plot_mapped, _plotclass = mmodel.predict(plot_input);\n",
    "            plot_current_map(tf.convert_to_tensor(plot_mapped, dtype=tf.float32));\n",
    "    \n",
    "#     del mmodel, mapping_layer;\n",
    "    \n",
    "    return res_map.squeeze();\n",
    "#     onoise = np.random.random((1, g_input_size));\n",
    "    \n",
    "#     return construct_map_with_sliders(tf.convert_to_tensor(gmodel.predict(onoise)), extvar=extvar).numpy().squeeze();\n",
    "\n",
    "# generate the map (main function)\n",
    "# dist_multiplier in #6 is used here\n",
    "def generate_map():\n",
    "    o = [];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    pos = [np.random.randint(100, 412), np.random.randint(80, 304)];\n",
    "    models = make_models();\n",
    "    \n",
    "    print(\"# of groups: {}\".format(timestamps.shape[0] // note_group_size));\n",
    "    for i in range(timestamps.shape[0] // note_group_size):\n",
    "        z = generate_set(models, begin = i * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = i, plot_map=False) * np.array([512, 384, 1, 1, 512, 384]);\n",
    "        pos = z[-1, 0:2];\n",
    "        o.append(z);\n",
    "    a = np.concatenate(o, axis=0);\n",
    "    return a;\n",
    "\n",
    "# generate a test map (debugging function)\n",
    "# dist_multiplier in #6 is used here\n",
    "def generate_test():\n",
    "    o = [];\n",
    "    pos = [384, 288];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    generate_set(begin = 3 * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = 3, plot_map=True);\n",
    "\n",
    "# for debugging only! it should be sent to node load_map.js c instead.\n",
    "def print_osu_text(a):\n",
    "    for i, ai in enumerate(a):\n",
    "        if not is_slider[i]:\n",
    "            print(\"{},{},{},1,0,0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i])));\n",
    "        else:\n",
    "            print(\"{},{},{},2,0,L|{}:{},1,{},0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i]), int(round(ai[0] + ai[2] * slider_lengths[i])), int(round(ai[1] + ai[3] * slider_lengths[i])), int(slider_length_base[i] * slider_ticks[i])));\n",
    "    \n",
    "osu_a = generate_map();\n",
    "# generate_test();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the generated flow data to a dict, and mix it into the JSON file converted from .osu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this needs to be JSON serializable, so we need to carefully convert the types.\n",
    "# Numpy somehow got its own types like numpy.int64 that does not allow the data to be serialized to JSON.\n",
    "def convert_to_osu_obj(obj_array):\n",
    "    output = [];\n",
    "    for i, obj in enumerate(obj_array):\n",
    "        if not is_slider[i]: # is a circle; does not consider spinner for now.\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 1,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"index\": i\n",
    "            };\n",
    "        else:\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 2,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"sliderGenerator\": {\n",
    "                    \"type\": int(slider_types[i]),\n",
    "                    \"dOut\": [float(obj[2]), float(obj[3])],\n",
    "                    \"len\": float(slider_length_base[i] * slider_ticks[i]),\n",
    "                    \"ticks\": int(slider_ticks[i]),\n",
    "                    \"endpoint\": [int(obj[4]), int(obj[5])]\n",
    "                },\n",
    "                \"index\": i\n",
    "            };\n",
    "        output.append(obj_dict);\n",
    "    return output;\n",
    "\n",
    "def get_osu_file_name(metadata):\n",
    "    artist = metadata[\"artist\"];\n",
    "    title = metadata[\"title\"];\n",
    "    creator = metadata[\"creator\"];\n",
    "    diffname = metadata[\"diffname\"];\n",
    "    outname = (artist+\" - \" if len(artist) > 0 else \"\") + title + \" (\" + creator + \") [\" + diffname + \"].osu\";\n",
    "    outname = re.sub(\"[^a-zA-Z0-9\\(\\)\\[\\] \\.\\,\\!\\~\\`\\{\\}\\-\\_\\=\\+\\&\\^\\@\\#\\$\\%\\;\\']\",\"\", outname);\n",
    "    return outname;\n",
    "\n",
    "osu_obj_array = convert_to_osu_obj(osu_a);\n",
    "\n",
    "with open(\"mapthis.json\", encoding=\"utf-8\") as map_json:\n",
    "    map_dict = json.load(map_json);\n",
    "    map_meta = map_dict[\"meta\"];\n",
    "    filename = get_osu_file_name(map_meta);\n",
    "    map_dict[\"obj\"] = osu_obj_array;\n",
    "\n",
    "with open('mapthis.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(map_dict, outfile, ensure_ascii=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, ask node.js to convert the JSON back to a .osu file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success! finished on: 2020-07-30 16:22:11.359618\n"
     ]
    }
   ],
   "source": [
    "subprocess.call([\"node\", \"load_map.js\", \"c\", \"mapthis.json\", filename]);\n",
    "print(\"success! finished on: {}\".format(datetime.now()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPnIVuaSJwWz"
   },
   "source": [
    "If it works alright, you should have a nice .osu file under the folder of these notebooks now!\n",
    "\n",
    "If it does not work, please tell me the problem so probably I could fix it!\n",
    "\n",
    "@2019/6/29"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
